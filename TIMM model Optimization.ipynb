{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brainimage/anaconda3/envs/brainimage/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'swin_small_patch4_window7_224'\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=1000, in_chans=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/brainimage/filters/val/ILSVRC2012_val_00000001.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000002.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000003.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000004.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000005.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000006.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000007.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000008.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000009.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000010.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000011.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000012.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000013.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000014.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000015.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000016.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000017.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000018.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000019.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000020.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000021.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000022.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000023.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000024.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000025.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000026.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000027.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000028.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000029.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000030.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000031.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000032.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000033.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000034.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000035.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000036.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000037.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000038.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000039.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000040.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000041.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000042.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000043.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000044.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000045.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000046.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000047.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000048.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000049.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000050.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000051.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000052.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000053.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000054.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000055.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000056.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000057.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000058.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000059.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000060.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000061.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000062.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000063.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000064.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000065.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000066.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000067.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000068.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000069.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000070.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000071.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000072.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000073.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000074.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000075.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000076.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000077.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000078.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000079.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000080.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000081.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000082.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000083.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000084.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000085.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000086.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000087.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000088.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000089.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000090.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000091.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000092.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000093.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000094.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000095.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000096.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000097.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000098.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000099.JPEG', '/home/brainimage/filters/val/ILSVRC2012_val_00000100.JPEG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.29it/s]\n",
      "/tmp/ipykernel_336109/404962316.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = torch.tensor(images)\n"
     ]
    }
   ],
   "source": [
    "# define the transformation to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize the image to match the model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "images = natsorted(glob('/home/brainimage/filters/val/*'))\n",
    "labels = open('./labels.txt', 'r').readlines()\n",
    "\n",
    "print(images[:100])\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    images[i] = Image.open(images[i])\n",
    "    images[i] = images[i].convert('RGB')\n",
    "    images[i] = transform(images[i])\n",
    "    labels[i] = int(labels[i].strip().split(' ')[1])\n",
    "\n",
    "images = images[:100]\n",
    "images = torch.stack(images)\n",
    "labels = labels[:100]\n",
    "\n",
    "images = torch.tensor(images)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(images, labels)\n",
    "\n",
    "# create the test dataset\n",
    "test_dataset = torch.utils.data.TensorDataset(images, labels)\n",
    "\n",
    "## perfrom slicing of the dataset\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, range(100))\n",
    "\n",
    "# create the test loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 65, 795, 230, 809, 520,  60, 334, 415, 674, 332, 109, 286, 370, 757,\n",
      "        595, 147, 327,  23, 478, 517, 334, 208, 948, 727,  23, 846, 270, 166,\n",
      "         55, 538, 324, 573, 360, 981, 586, 887,  26, 398, 777,  74, 431, 756,\n",
      "        129, 198, 256, 725, 565, 166, 717, 467,  92,  29, 844, 591, 359, 468,\n",
      "        154, 994, 872, 588, 735, 197, 107,  46, 842, 390, 101, 887, 870, 911,\n",
      "          4, 149,  21, 476,  80, 424, 159, 275, 175, 461, 970, 160, 788,  58,\n",
      "        479, 498, 368,  28, 487,  50, 270, 383, 366, 780, 373, 705, 330, 142,\n",
      "        949, 349])\n",
      "Accuracy of the network on the 100 test images: 81 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            print(predicted)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 100 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "evaluate(quantized_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(), 'quantized_model.pth')\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:27<00:00, 27.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 65, 795, 230, 809, 520,  60, 334, 415, 674, 332, 109, 286, 370, 757,\n",
      "        595, 147, 327,  23, 478, 517, 334, 208, 948, 727,  23, 846, 270, 166,\n",
      "         55, 538, 324, 573, 360, 981, 586, 887,  26, 398, 777,  74, 431, 756,\n",
      "        129, 198, 256, 725, 565, 166, 717, 467,  92,  29, 844, 591, 359, 468,\n",
      "        154, 994, 872, 588, 735, 197, 107,  46, 842, 390, 101, 887, 870, 911,\n",
      "          4, 149,  21, 476,  80, 424, 159, 275, 175, 461, 970, 160, 788,  58,\n",
      "        479, 498, 368,  28, 487,  50, 270, 383, 366, 780, 373, 705, 330, 142,\n",
      "        949, 349])\n",
      "Accuracy of the network on the 100 test images: 81 %\n",
      "Original model size: 200110797 bytes\n",
      "Quantized model size: 51924361 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the original model\n",
    "evaluate(model)\n",
    "\n",
    "# Get the size of the original model\n",
    "orig_size = os.path.getsize('model.pth')\n",
    "print(f\"Original model size: {orig_size} bytes\")\n",
    "\n",
    "# Get the size of the quantized model\n",
    "quantized_size = os.path.getsize('quantized_model.pth')\n",
    "print(f\"Quantized model size: {quantized_size} bytes\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As an advanced task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\")\n",
    "    os.remove(\"temp.p\")\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 65, 795, 230, 809, 520,  60, 334, 415, 674, 332, 109, 286, 370, 757,\n",
      "        595, 147, 327,  23, 478, 517, 334, 208, 948, 727,  23, 846, 270, 166,\n",
      "         55, 538, 324, 573, 360, 981, 586, 887,  26, 398, 777,  74, 431, 756,\n",
      "        129, 198, 256, 725, 565, 166, 717, 467,  92,  29, 844, 591, 359, 468,\n",
      "        154, 994, 872, 588, 735, 197, 107,  46, 842, 390, 101, 887, 870, 911,\n",
      "          4, 149,  21, 476,  80, 424, 159, 275, 175, 461, 970, 160, 788,  58,\n",
      "        479, 498, 368,  28, 487,  50, 270, 383, 366, 780, 373, 705, 330, 142,\n",
      "        949, 349])\n",
      "Accuracy of the network on the 100 test images: 81 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 51919308\n"
     ]
    }
   ],
   "source": [
    "def quantize_model(model):\n",
    "    # Set quantization parameters for activations and weights\n",
    "    activation_observer = torch.quantization.observer.HistogramObserver.with_args(dtype=torch.quint4x2, qscheme=torch.per_tensor_affine)\n",
    "    quantization_config = torch.quantization.QConfig(activation=activation_observer, weight=torch.quantization.default_weight_observer)\n",
    "    # Prepare the model for quantization\n",
    "    model.qconfig = quantization_config\n",
    "    torch.backends.quantized.engine = 'qnnpack'\n",
    "    model_prepared = torch.quantization.prepare(model, inplace=True)\n",
    "    # Calibrate the model to find appropriate quantization parameters\n",
    "    # with torch.no_grad():\n",
    "    #     for images, labels in test_loader:\n",
    "    #         model_prepared(images)\n",
    "    evaluate(model_prepared)\n",
    "    # Convert the model to a quantized model\n",
    "    model_quantized = torch.quantization.convert(model_prepared, inplace=True)\n",
    "    # Print model size\n",
    "    print(f\"Quantized model size: {get_model_size(model_quantized)}\")\n",
    "    return model_quantized\n",
    "\n",
    "quantized_model = quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainimage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
